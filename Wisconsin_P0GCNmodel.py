import torch
import ptens
from torch_geometric.transforms import NormalizeFeatures
from torch_geometric.transforms.random_node_split import RandomNodeSplit
from Transforms import ToPtens_Batch
from torch_geometric.datasets import WebKB
dataset = WebKB(root='data/WebKB', name='Wisconsin', transform=NormalizeFeatures())
data = dataset[0]
transform_nodes = RandomNodeSplit(split = 'random', 
                                 num_train_per_class = 100,
                                 num_val = 1, 
                                 num_test = 151)
data = transform_nodes(data)
on_learn_transform = ToPtens_Batch()
data = on_learn_transform(data)


class P0GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super().__init__()
        torch.manual_seed(12345)
        self.conv1 = ptens.modules.ConvolutionalLayer_0P(dataset.num_features, hidden_channels)
        self.conv2 = ptens.modules.ConvolutionalLayer_0P(hidden_channels, dataset.num_classes)
        self.dropout = ptens.modules.Dropout(prob=0.5, device = None)

    def forward(self, x, edge_index):
        x = self.conv1(x,edge_index)
        x = x.relu()
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return x

def train():
      model.train()
      optimizer.zero_grad()
      out = model(ptens.ptensors0.from_matrix(data.x),data.G).torch()
      loss = criterion(out[data.train_mask], data.y[data.train_mask])  
      loss.backward() 
      optimizer.step() 
      return loss
def test():
      model.eval()
      out = model(ptens.ptensors0.from_matrix(data.x),data.G).torch()
      pred = out.argmax(dim=1)  
      train_correct = pred[data.train_mask] == data.y[data.train_mask]  
      train_acc = int(train_correct.sum()) / int(data.train_mask.sum())  
      test_correct = pred[data.test_mask] == data.y[data.test_mask]  
      test_acc = int(test_correct.sum()) / int(data.test_mask.sum()) 
      return train_acc, test_acc

    
model = P0GCN(hidden_channels = 64) # subject to change
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
criterion = torch.nn.CrossEntropyLoss()
ls = []
tr_ac = []
te_ac = []
for epoch in range(1, 201):
    loss = train()
    train_acc, test_acc = test()
    ls.append(loss)
    tr_ac.append(train_acc)
    te_ac.append(test_acc)
    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')
print("Train Accuracy:", train_acc, ". Test Accuracy:", test_acc, ".")
print("Loss:", ls)
print("Train Accuracys:", tr_ac)
print("Test Accuracys:", te_ac)
print('=================================================================')
"""
Wisconsin:
num_train_per_class = 100, num_val = 1, num_test = 150
hidden_channels = 64
epoches = 200
Train Accuracy: 0.6909871244635193 . Test Accuracy: 0.8235294117647058 .
Loss: [tensor(1.6103, grad_fn=<NllLossBackward0>), tensor(1.5764, grad_fn=<NllLossBackward0>), tensor(1.5516, grad_fn=<NllLossBackward0>), tensor(1.5297, grad_fn=<NllLossBackward0>), tensor(1.5146, grad_fn=<NllLossBackward0>), tensor(1.4830, grad_fn=<NllLossBackward0>), tensor(1.4423, grad_fn=<NllLossBackward0>), tensor(1.4460, grad_fn=<NllLossBackward0>), tensor(1.4550, grad_fn=<NllLossBackward0>), tensor(1.4035, grad_fn=<NllLossBackward0>), tensor(1.4025, grad_fn=<NllLossBackward0>), tensor(1.3680, grad_fn=<NllLossBackward0>), tensor(1.3615, grad_fn=<NllLossBackward0>), tensor(1.3939, grad_fn=<NllLossBackward0>), tensor(1.3860, grad_fn=<NllLossBackward0>), tensor(1.3660, grad_fn=<NllLossBackward0>), tensor(1.3327, grad_fn=<NllLossBackward0>), tensor(1.3214, grad_fn=<NllLossBackward0>), tensor(1.3092, grad_fn=<NllLossBackward0>), tensor(1.3017, grad_fn=<NllLossBackward0>), tensor(1.3146, grad_fn=<NllLossBackward0>), tensor(1.3112, grad_fn=<NllLossBackward0>), tensor(1.2827, grad_fn=<NllLossBackward0>), tensor(1.2941, grad_fn=<NllLossBackward0>), tensor(1.2676, grad_fn=<NllLossBackward0>), tensor(1.2926, grad_fn=<NllLossBackward0>), tensor(1.2654, grad_fn=<NllLossBackward0>), tensor(1.3830, grad_fn=<NllLossBackward0>), tensor(1.2270, grad_fn=<NllLossBackward0>), tensor(1.2694, grad_fn=<NllLossBackward0>), tensor(1.2476, grad_fn=<NllLossBackward0>), tensor(1.2383, grad_fn=<NllLossBackward0>), tensor(1.2620, grad_fn=<NllLossBackward0>), tensor(1.1948, grad_fn=<NllLossBackward0>), tensor(1.1929, grad_fn=<NllLossBackward0>), tensor(1.1993, grad_fn=<NllLossBackward0>), tensor(1.1835, grad_fn=<NllLossBackward0>), tensor(1.1798, grad_fn=<NllLossBackward0>), tensor(1.1628, grad_fn=<NllLossBackward0>), tensor(1.2136, grad_fn=<NllLossBackward0>), tensor(1.1738, grad_fn=<NllLossBackward0>), tensor(1.2571, grad_fn=<NllLossBackward0>), tensor(1.1615, grad_fn=<NllLossBackward0>), tensor(1.1619, grad_fn=<NllLossBackward0>), tensor(1.1159, grad_fn=<NllLossBackward0>), tensor(1.1724, grad_fn=<NllLossBackward0>), tensor(1.1486, grad_fn=<NllLossBackward0>), tensor(1.1527, grad_fn=<NllLossBackward0>), tensor(1.1393, grad_fn=<NllLossBackward0>), tensor(1.1721, grad_fn=<NllLossBackward0>), tensor(1.1126, grad_fn=<NllLossBackward0>), tensor(1.1773, grad_fn=<NllLossBackward0>), tensor(1.1086, grad_fn=<NllLossBackward0>), tensor(1.0959, grad_fn=<NllLossBackward0>), tensor(1.1141, grad_fn=<NllLossBackward0>), tensor(1.1043, grad_fn=<NllLossBackward0>), tensor(1.0566, grad_fn=<NllLossBackward0>), tensor(1.0750, grad_fn=<NllLossBackward0>), tensor(1.1038, grad_fn=<NllLossBackward0>), tensor(1.1164, grad_fn=<NllLossBackward0>), tensor(1.1281, grad_fn=<NllLossBackward0>), tensor(1.0540, grad_fn=<NllLossBackward0>), tensor(1.0622, grad_fn=<NllLossBackward0>), tensor(1.0757, grad_fn=<NllLossBackward0>), tensor(1.0517, grad_fn=<NllLossBackward0>), tensor(1.1450, grad_fn=<NllLossBackward0>), tensor(1.1061, grad_fn=<NllLossBackward0>), tensor(1.0838, grad_fn=<NllLossBackward0>), tensor(1.1609, grad_fn=<NllLossBackward0>), tensor(1.0588, grad_fn=<NllLossBackward0>), tensor(1.0731, grad_fn=<NllLossBackward0>), tensor(1.2050, grad_fn=<NllLossBackward0>), tensor(1.1220, grad_fn=<NllLossBackward0>), tensor(1.0825, grad_fn=<NllLossBackward0>), tensor(1.0542, grad_fn=<NllLossBackward0>), tensor(1.0365, grad_fn=<NllLossBackward0>), tensor(1.0534, grad_fn=<NllLossBackward0>), tensor(1.1095, grad_fn=<NllLossBackward0>), tensor(1.0205, grad_fn=<NllLossBackward0>), tensor(1.1080, grad_fn=<NllLossBackward0>), tensor(1.0274, grad_fn=<NllLossBackward0>), tensor(1.0177, grad_fn=<NllLossBackward0>), tensor(0.9932, grad_fn=<NllLossBackward0>), tensor(1.1339, grad_fn=<NllLossBackward0>), tensor(1.0504, grad_fn=<NllLossBackward0>), tensor(1.0753, grad_fn=<NllLossBackward0>), tensor(1.0828, grad_fn=<NllLossBackward0>), tensor(1.0145, grad_fn=<NllLossBackward0>), tensor(1.0559, grad_fn=<NllLossBackward0>), tensor(1.0869, grad_fn=<NllLossBackward0>), tensor(1.0315, grad_fn=<NllLossBackward0>), tensor(1.0786, grad_fn=<NllLossBackward0>), tensor(1.0904, grad_fn=<NllLossBackward0>), tensor(1.0858, grad_fn=<NllLossBackward0>), tensor(1.0490, grad_fn=<NllLossBackward0>), tensor(1.0112, grad_fn=<NllLossBackward0>), tensor(1.0628, grad_fn=<NllLossBackward0>), tensor(0.9876, grad_fn=<NllLossBackward0>), tensor(1.1048, grad_fn=<NllLossBackward0>), tensor(1.0331, grad_fn=<NllLossBackward0>), tensor(1.0406, grad_fn=<NllLossBackward0>), tensor(1.0154, grad_fn=<NllLossBackward0>), tensor(1.0704, grad_fn=<NllLossBackward0>), tensor(1.0465, grad_fn=<NllLossBackward0>), tensor(0.9906, grad_fn=<NllLossBackward0>), tensor(1.0619, grad_fn=<NllLossBackward0>), tensor(1.0327, grad_fn=<NllLossBackward0>), tensor(1.0126, grad_fn=<NllLossBackward0>), tensor(1.0376, grad_fn=<NllLossBackward0>), tensor(1.0790, grad_fn=<NllLossBackward0>), tensor(1.0044, grad_fn=<NllLossBackward0>), tensor(1.0765, grad_fn=<NllLossBackward0>), tensor(0.9675, grad_fn=<NllLossBackward0>), tensor(0.9858, grad_fn=<NllLossBackward0>), tensor(0.9438, grad_fn=<NllLossBackward0>), tensor(0.9847, grad_fn=<NllLossBackward0>), tensor(0.9627, grad_fn=<NllLossBackward0>), tensor(1.0963, grad_fn=<NllLossBackward0>), tensor(1.0156, grad_fn=<NllLossBackward0>), tensor(0.9561, grad_fn=<NllLossBackward0>), tensor(0.9964, grad_fn=<NllLossBackward0>), tensor(0.9522, grad_fn=<NllLossBackward0>), tensor(1.0625, grad_fn=<NllLossBackward0>), tensor(1.0056, grad_fn=<NllLossBackward0>), tensor(0.9443, grad_fn=<NllLossBackward0>), tensor(1.0057, grad_fn=<NllLossBackward0>), tensor(1.0150, grad_fn=<NllLossBackward0>), tensor(1.0035, grad_fn=<NllLossBackward0>), tensor(0.9562, grad_fn=<NllLossBackward0>), tensor(0.9417, grad_fn=<NllLossBackward0>), tensor(0.9300, grad_fn=<NllLossBackward0>), tensor(0.9614, grad_fn=<NllLossBackward0>), tensor(0.9602, grad_fn=<NllLossBackward0>), tensor(1.0203, grad_fn=<NllLossBackward0>), tensor(0.9341, grad_fn=<NllLossBackward0>), tensor(0.9730, grad_fn=<NllLossBackward0>), tensor(0.9195, grad_fn=<NllLossBackward0>), tensor(0.9720, grad_fn=<NllLossBackward0>), tensor(1.0024, grad_fn=<NllLossBackward0>), tensor(0.9800, grad_fn=<NllLossBackward0>), tensor(1.1208, grad_fn=<NllLossBackward0>), tensor(0.8984, grad_fn=<NllLossBackward0>), tensor(0.9402, grad_fn=<NllLossBackward0>), tensor(0.9411, grad_fn=<NllLossBackward0>), tensor(0.9228, grad_fn=<NllLossBackward0>), tensor(0.9489, grad_fn=<NllLossBackward0>), tensor(0.9618, grad_fn=<NllLossBackward0>), tensor(0.9843, grad_fn=<NllLossBackward0>), tensor(0.9348, grad_fn=<NllLossBackward0>), tensor(0.9434, grad_fn=<NllLossBackward0>), tensor(0.9450, grad_fn=<NllLossBackward0>), tensor(0.9740, grad_fn=<NllLossBackward0>), tensor(0.9591, grad_fn=<NllLossBackward0>), tensor(0.9893, grad_fn=<NllLossBackward0>), tensor(0.9604, grad_fn=<NllLossBackward0>), tensor(0.9695, grad_fn=<NllLossBackward0>), tensor(0.9641, grad_fn=<NllLossBackward0>), tensor(0.9895, grad_fn=<NllLossBackward0>), tensor(0.9371, grad_fn=<NllLossBackward0>), tensor(1.0030, grad_fn=<NllLossBackward0>), tensor(0.9498, grad_fn=<NllLossBackward0>), tensor(0.8999, grad_fn=<NllLossBackward0>), tensor(0.9167, grad_fn=<NllLossBackward0>), tensor(0.9847, grad_fn=<NllLossBackward0>), tensor(0.9255, grad_fn=<NllLossBackward0>), tensor(0.9176, grad_fn=<NllLossBackward0>), tensor(1.0523, grad_fn=<NllLossBackward0>), tensor(0.9258, grad_fn=<NllLossBackward0>), tensor(1.0603, grad_fn=<NllLossBackward0>), tensor(0.9663, grad_fn=<NllLossBackward0>), tensor(0.9214, grad_fn=<NllLossBackward0>), tensor(0.9269, grad_fn=<NllLossBackward0>), tensor(0.9210, grad_fn=<NllLossBackward0>), tensor(0.9127, grad_fn=<NllLossBackward0>), tensor(0.9038, grad_fn=<NllLossBackward0>), tensor(0.9144, grad_fn=<NllLossBackward0>), tensor(0.9233, grad_fn=<NllLossBackward0>), tensor(1.0306, grad_fn=<NllLossBackward0>), tensor(0.8823, grad_fn=<NllLossBackward0>), tensor(0.8917, grad_fn=<NllLossBackward0>), tensor(1.0783, grad_fn=<NllLossBackward0>), tensor(0.9609, grad_fn=<NllLossBackward0>), tensor(0.9366, grad_fn=<NllLossBackward0>), tensor(1.0151, grad_fn=<NllLossBackward0>), tensor(1.0409, grad_fn=<NllLossBackward0>), tensor(0.9985, grad_fn=<NllLossBackward0>), tensor(0.9461, grad_fn=<NllLossBackward0>), tensor(0.9450, grad_fn=<NllLossBackward0>), tensor(1.0611, grad_fn=<NllLossBackward0>), tensor(0.9059, grad_fn=<NllLossBackward0>), tensor(0.9090, grad_fn=<NllLossBackward0>), tensor(0.9285, grad_fn=<NllLossBackward0>), tensor(1.0321, grad_fn=<NllLossBackward0>), tensor(0.9002, grad_fn=<NllLossBackward0>), tensor(0.9033, grad_fn=<NllLossBackward0>), tensor(0.9116, grad_fn=<NllLossBackward0>), tensor(0.9372, grad_fn=<NllLossBackward0>), tensor(0.9005, grad_fn=<NllLossBackward0>), tensor(0.9448, grad_fn=<NllLossBackward0>), tensor(0.8866, grad_fn=<NllLossBackward0>)]
Train Accuracys: [0.30042918454935624, 0.4892703862660944, 0.4892703862660944, 0.4892703862660944, 0.4892703862660944, 0.4892703862660944, 0.5622317596566524, 0.5364806866952789, 0.5107296137339056, 0.6051502145922747, 0.5965665236051502, 0.5236051502145923, 0.4978540772532189, 0.5407725321888412, 0.51931330472103, 0.51931330472103, 0.6394849785407726, 0.5064377682403434, 0.4892703862660944, 0.5236051502145923, 0.6180257510729614, 0.6223175965665236, 0.6051502145922747, 0.630901287553648, 0.5793991416309013, 0.6008583690987125, 0.6180257510729614, 0.6223175965665236, 0.6223175965665236, 0.6223175965665236, 0.5879828326180258, 0.6394849785407726, 0.6180257510729614, 0.6137339055793991, 0.6394849785407726, 0.6137339055793991, 0.6394849785407726, 0.630901287553648, 0.5965665236051502, 0.6523605150214592, 0.6523605150214592, 0.6566523605150214, 0.6566523605150214, 0.6051502145922747, 0.6094420600858369, 0.6566523605150214, 0.6437768240343348, 0.630901287553648, 0.5836909871244635, 0.6180257510729614, 0.630901287553648, 0.6394849785407726, 0.5879828326180258, 0.6523605150214592, 0.5879828326180258, 0.6351931330472103, 0.6738197424892703, 0.6738197424892703, 0.6695278969957081, 0.5407725321888412, 0.6523605150214592, 0.6738197424892703, 0.6523605150214592, 0.648068669527897, 0.51931330472103, 0.6051502145922747, 0.6394849785407726, 0.630901287553648, 0.6266094420600858, 0.6609442060085837, 0.6008583690987125, 0.5879828326180258, 0.630901287553648, 0.648068669527897, 0.6051502145922747, 0.6609442060085837, 0.6652360515021459, 0.6180257510729614, 0.630901287553648, 0.6394849785407726, 0.648068669527897, 0.6652360515021459, 0.6695278969957081, 0.630901287553648, 0.6266094420600858, 0.6523605150214592, 0.6094420600858369, 0.6437768240343348, 0.6137339055793991, 0.6266094420600858, 0.6223175965665236, 0.6394849785407726, 0.6437768240343348, 0.6437768240343348, 0.630901287553648, 0.6223175965665236, 0.6437768240343348, 0.6738197424892703, 0.6781115879828327, 0.5536480686695279, 0.648068669527897, 0.6652360515021459, 0.6351931330472103, 0.5965665236051502, 0.648068669527897, 0.6351931330472103, 0.6566523605150214, 0.6437768240343348, 0.6866952789699571, 0.6952789699570815, 0.6137339055793991, 0.6008583690987125, 0.6566523605150214, 0.6824034334763949, 0.6738197424892703, 0.6824034334763949, 0.6952789699570815, 0.6609442060085837, 0.6695278969957081, 0.6909871244635193, 0.6652360515021459, 0.648068669527897, 0.6394849785407726, 0.6437768240343348, 0.6394849785407726, 0.5536480686695279, 0.6351931330472103, 0.6351931330472103, 0.6180257510729614, 0.6824034334763949, 0.6394849785407726, 0.6824034334763949, 0.6008583690987125, 0.6523605150214592, 0.648068669527897, 0.6180257510729614, 0.6437768240343348, 0.6609442060085837, 0.6738197424892703, 0.6437768240343348, 0.630901287553648, 0.5965665236051502, 0.6266094420600858, 0.6051502145922747, 0.6609442060085837, 0.6781115879828327, 0.6394849785407726, 0.6952789699570815, 0.6394849785407726, 0.6437768240343348, 0.6437768240343348, 0.648068669527897, 0.703862660944206, 0.6566523605150214, 0.6652360515021459, 0.592274678111588, 0.6223175965665236, 0.6266094420600858, 0.6566523605150214, 0.6609442060085837, 0.6394849785407726, 0.7081545064377682, 0.6566523605150214, 0.7124463519313304, 0.6866952789699571, 0.6952789699570815, 0.648068669527897, 0.6781115879828327, 0.648068669527897, 0.648068669527897, 0.6695278969957081, 0.6824034334763949, 0.6781115879828327, 0.6566523605150214, 0.6394849785407726, 0.6609442060085837, 0.6094420600858369, 0.6995708154506438, 0.6394849785407726, 0.6609442060085837, 0.6952789699570815, 0.7081545064377682, 0.6652360515021459, 0.7081545064377682, 0.6437768240343348, 0.6609442060085837, 0.6394849785407726, 0.6695278969957081, 0.6781115879828327, 0.6866952789699571, 0.6652360515021459, 0.5879828326180258, 0.6609442060085837, 0.6824034334763949, 0.6566523605150214, 0.6266094420600858, 0.6437768240343348, 0.6824034334763949, 0.7124463519313304, 0.6909871244635193]
Test Accuracys: [0.0, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.5882352941176471, 0.5294117647058824, 0.47058823529411764, 0.9411764705882353, 0.5294117647058824, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.35294117647058826, 0.7058823529411765, 0.4117647058823529, 0.35294117647058826, 0.47058823529411764, 0.5882352941176471, 0.5294117647058824, 0.9411764705882353, 0.7058823529411765, 0.47058823529411764, 0.7647058823529411, 0.5882352941176471, 0.7058823529411765, 0.6470588235294118, 0.7058823529411765, 0.7058823529411765, 0.6470588235294118, 0.6470588235294118, 0.7058823529411765, 0.5294117647058824, 0.7647058823529411, 0.6470588235294118, 0.7058823529411765, 0.8823529411764706, 0.5294117647058824, 0.5882352941176471, 0.8823529411764706, 0.5294117647058824, 0.7647058823529411, 0.9411764705882353, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.8235294117647058, 0.7058823529411765, 0.8235294117647058, 0.7058823529411765, 0.5882352941176471, 0.6470588235294118, 0.8235294117647058, 0.7647058823529411, 0.8235294117647058, 0.5882352941176471, 0.7647058823529411, 1.0, 0.5294117647058824, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 1.0, 0.9411764705882353, 0.9411764705882353, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.5882352941176471, 0.8235294117647058, 0.8235294117647058, 0.5294117647058824, 0.9411764705882353, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.8823529411764706, 0.6470588235294118, 0.7647058823529411, 0.7647058823529411, 0.9411764705882353, 0.8235294117647058, 0.5882352941176471, 0.8235294117647058, 0.7647058823529411, 0.7058823529411765, 0.7058823529411765, 0.47058823529411764, 0.7647058823529411, 0.5294117647058824, 0.8235294117647058, 0.5294117647058824, 0.5882352941176471, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8823529411764706, 0.8823529411764706, 0.9411764705882353, 0.7647058823529411, 0.8823529411764706, 0.7647058823529411, 0.7058823529411765, 0.8235294117647058, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.8823529411764706, 0.8823529411764706, 0.7058823529411765, 0.7647058823529411, 0.7647058823529411, 0.5294117647058824, 0.7058823529411765, 0.7058823529411765, 0.7058823529411765, 0.5882352941176471, 0.8235294117647058, 0.7058823529411765, 0.8235294117647058, 0.8235294117647058, 0.7647058823529411, 1.0, 0.7647058823529411, 0.7647058823529411, 0.8235294117647058, 0.7058823529411765, 0.8235294117647058, 0.7058823529411765, 0.8235294117647058, 0.7647058823529411, 0.7647058823529411, 0.8823529411764706, 0.8235294117647058, 0.7058823529411765, 0.6470588235294118, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.9411764705882353, 0.8235294117647058, 0.8823529411764706, 0.7058823529411765, 0.8235294117647058, 0.8235294117647058, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.7647058823529411, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.7647058823529411, 0.6470588235294118, 0.8235294117647058, 0.9411764705882353, 0.6470588235294118, 0.7058823529411765, 0.8235294117647058, 0.6470588235294118, 0.5294117647058824, 0.7647058823529411, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.6470588235294118, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.7647058823529411, 0.7647058823529411, 0.7058823529411765, 0.8235294117647058, 0.8235294117647058, 0.8823529411764706, 0.7058823529411765, 1.0, 0.8235294117647058, 0.8235294117647058, 0.7058823529411765, 1.0, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058]

"""
